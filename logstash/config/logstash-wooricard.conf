# 입력 섹션: 데이터 소스 정의
input {
  # Beats(예: Filebeat)로부터 데이터 수신
  beats {
    port => 5044  # 5044 포트에서 Beats 데이터 수신
  }
}
# 필터 섹션: 수신된 데이터 처리 및 변환
filter {
  # Grok 필터: 로그 메시지를 구조화된 필드로 파싱
  grok {
    match => { "message" => "%{LOGLEVEL:loglevel}:getlog:%{GREEDYDATA:json_data}" }
  }
  # Ruby 필터: JSON 데이터 파싱 및 정제
  ruby {
    code => '
      require "json"
      # 작은따옴표를 큰따옴표로 변경하고 nan을 null로 변경
      json_str = event.get("json_data").gsub("\'", "\"").gsub("nan", "null")
      begin
        # JSON 파싱
        parsed = JSON.parse(json_str)
        # 파싱된 데이터를 customer_data 필드에 설정
        event.set("customer_data", parsed)
      rescue => e
        # 파싱 실패 시 태그 추가 및 에러 메시지 저장
        event.tag("_jsonparsefailure")
        event.set("json_parse_error", e.message)
      end
    '
  }
  # Mutate 필터: 필드 변환 및 제거
  mutate {
    # customer_data 내의 숫자 필드들을 정수형으로 변환
    convert => {
      #컬럼명 비공개 
    }
    
    # 불필요한 필드 제거
    remove_field => ["input", "log", "host", "agent", "@version", "event", "ecs", "tags",
      "json_data", "message"]
  }
}
# 출력 섹션: 처리된 데이터의 목적지 정의
output {
  # Elasticsearch로 데이터 전송
  elasticsearch {
    hosts => ["localhost:9200"]  # Elasticsearch 호스트 주소
    index => "customer_data-%{+YYYY.MM.dd}"  # 인덱스 이름 (날짜별로 생성)
  }
}